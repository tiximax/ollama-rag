"""
üî• Load Testing Master File - Sprint 3 Day 1
============================================

M·ª•c ƒë√≠ch: Stress-test h·ªá th·ªëng RAG v·ªõi Ollama, validate metrics th·ª±c chi·∫øn!

Features:
- Realistic user behavior simulation (query, chat, RAG retrieval)
- Multiple load patterns (normal, spike, stress)
- Live Prometheus metrics integration
- Circuit breaker & connection pool validation

Vibe: Testing nh∆∞ m·ªôt rockstar! üé∏
"""

import json
import random
import time

from locust import HttpUser, TaskSet, between, task

# ============================================================================
# üéØ Configuration - T√πy ch·ªânh theo nhu c·∫ßu stress test
# ============================================================================


class LoadTestConfig:
    """Configuration ·ªïn ƒë·ªãnh cho load tests nh∆∞ kim c∆∞∆°ng! üíé"""

    # Endpoints to test
    OLLAMA_GENERATE_ENDPOINT = "/api/generate"
    OLLAMA_CHAT_ENDPOINT = "/api/chat"
    METRICS_ENDPOINT = "/metrics"
    HEALTH_ENDPOINT = "/health"

    # Test data - Realistic queries
    SAMPLE_QUERIES = [
        "What is machine learning?",
        "Explain neural networks",
        "How does RAG work?",
        "Tell me about transformers",
        "What is semantic search?",
        "Explain vector databases",
        "How to optimize LLM inference?",
        "What is prompt engineering?",
        "Describe attention mechanism",
        "What are embeddings?",
    ]

    # Chat conversation templates
    CHAT_TEMPLATES = [
        {"role": "user", "content": "Hello! Can you help me understand {topic}?"},
        {"role": "assistant", "content": "Of course! I'd be happy to explain {topic}."},
        {"role": "user", "content": "Can you give me an example?"},
    ]

    # Model to use for testing
    TEST_MODEL = "llama2"  # Thay ƒë·ªïi theo model b·∫°n c√≥

    # Request timeouts (seconds)
    GENERATE_TIMEOUT = 30
    CHAT_TIMEOUT = 30
    METRICS_TIMEOUT = 5


# ============================================================================
# üé≠ User Behaviors - M√¥ ph·ªèng h√†nh vi ng∆∞·ªùi d√πng th·ª±c t·∫ø
# ============================================================================


class OllamaUserBehavior(TaskSet):
    """
    H√†nh vi user chu·∫©n ch·ªânh: query ‚Üí check metrics ‚Üí chat

    M√¥ ph·ªèng workflow th·ª±c t·∫ø c·ªßa user t∆∞∆°ng t√°c v·ªõi RAG system!
    """

    def on_start(self):
        """
        Kh·ªüi t·∫°o khi user b·∫Øt ƒë·∫ßu session.
        Nh∆∞ l√≠nh canh, setup m·ªçi th·ª© tr∆∞·ªõc khi v√†o tr·∫≠n! üö™
        """
        self.user_id = f"user_{random.randint(1000, 9999)}"
        self.session_start = time.time()
        print(f"üöÄ User {self.user_id} started session")

    @task(5)
    def generate_query(self):
        """
        Task ch√≠nh: Generate response t·ª´ query (weight=5)

        ƒê√¢y l√† task ph·ªï bi·∫øn nh·∫•t, chi·∫øm 50% traffic!
        """
        query = random.choice(LoadTestConfig.SAMPLE_QUERIES)

        payload = {
            "model": LoadTestConfig.TEST_MODEL,
            "prompt": query,
            "stream": False,  # Kh√¥ng stream ƒë·ªÉ d·ªÖ ƒëo latency
        }

        with self.client.post(
            LoadTestConfig.OLLAMA_GENERATE_ENDPOINT,
            json=payload,
            timeout=LoadTestConfig.GENERATE_TIMEOUT,
            catch_response=True,
            name="Ollama Generate",
        ) as response:
            if response.status_code == 200:
                try:
                    data = response.json()
                    # Validate response c√≥ ƒë√∫ng format kh√¥ng
                    if "response" in data:
                        response.success()
                        print(f"‚úÖ {self.user_id}: Query successful - {query[:30]}...")
                    else:
                        response.failure("Missing 'response' field in JSON")
                except json.JSONDecodeError:
                    response.failure("Invalid JSON response")
            elif response.status_code == 503:
                # Circuit breaker OPEN - Expected behavior!
                print(f"‚ö° {self.user_id}: Circuit breaker triggered (503)")
                response.success()  # Kh√¥ng t√≠nh l√† l·ªói, l√† feature!
            else:
                response.failure(f"HTTP {response.status_code}")

    @task(3)
    def chat_conversation(self):
        """
        Task chat: Multi-turn conversation (weight=3)

        M√¥ ph·ªèng cu·ªôc tr√≤ chuy·ªán nhi·ªÅu l∆∞·ª£t v·ªõi context!
        """
        topic = random.choice(["AI", "ML", "NLP", "RAG", "LLMs"])

        # Build conversation from template
        messages = [
            {"role": msg["role"], "content": msg["content"].format(topic=topic)}
            for msg in LoadTestConfig.CHAT_TEMPLATES
        ]

        payload = {
            "model": LoadTestConfig.TEST_MODEL,
            "messages": messages,
            "stream": False,
        }

        with self.client.post(
            LoadTestConfig.OLLAMA_CHAT_ENDPOINT,
            json=payload,
            timeout=LoadTestConfig.CHAT_TIMEOUT,
            catch_response=True,
            name="Ollama Chat",
        ) as response:
            if response.status_code == 200:
                try:
                    data = response.json()
                    if "message" in data:
                        response.success()
                        print(f"üí¨ {self.user_id}: Chat successful about {topic}")
                    else:
                        response.failure("Missing 'message' field")
                except json.JSONDecodeError:
                    response.failure("Invalid JSON response")
            elif response.status_code == 503:
                print(f"‚ö° {self.user_id}: Circuit breaker triggered in chat")
                response.success()
            else:
                response.failure(f"HTTP {response.status_code}")

    @task(2)
    def check_metrics(self):
        """
        Task metrics check (weight=2)

        User ho·∫∑c monitoring system check metrics ƒë·ªãnh k·ª≥!
        """
        with self.client.get(
            LoadTestConfig.METRICS_ENDPOINT,
            timeout=LoadTestConfig.METRICS_TIMEOUT,
            catch_response=True,
            name="Metrics Check",
        ) as response:
            if response.status_code == 200:
                # Validate Prometheus format
                if "ollama_requests_total" in response.text:
                    response.success()
                    print(f"üìä {self.user_id}: Metrics checked")
                else:
                    response.failure("Invalid Prometheus metrics format")
            else:
                response.failure(f"HTTP {response.status_code}")

    @task(1)
    def health_check(self):
        """
        Task health check (weight=1)

        Load balancer ho·∫∑c monitoring check health!
        """
        with self.client.get(
            LoadTestConfig.HEALTH_ENDPOINT, timeout=5, catch_response=True, name="Health Check"
        ) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"HTTP {response.status_code}")


# ============================================================================
# üë§ User Classes - C√°c lo·∫°i user v·ªõi behavior kh√°c nhau
# ============================================================================


class NormalUser(HttpUser):
    """
    User b√¨nh th∆∞·ªùng: Query ·ªïn ƒë·ªãnh, wait time h·ª£p l√Ω

    ƒê√¢y l√† 80% traffic th·ª±c t·∫ø - user t∆∞∆°ng t√°c t·ª± nhi√™n! üéØ
    """

    tasks = [OllamaUserBehavior]
    wait_time = between(2, 5)  # Wait 2-5s gi·ªØa c√°c request (realistic!)
    weight = 8  # 80% users l√† normal


class PowerUser(HttpUser):
    """
    Power user: Query nhi·ªÅu h∆°n, wait time ng·∫Øn h∆°n

    Developer ho·∫∑c heavy user - 15% traffic! üí™
    """

    tasks = [OllamaUserBehavior]
    wait_time = between(0.5, 2)  # Nhanh h∆°n g·∫•p 2-3 l·∫ßn
    weight = 2  # 15% users


class SpikeUser(HttpUser):
    """
    Spike user: Burst traffic ƒë·ªôt ng·ªôt, no wait time

    Simulate traffic spike - 5% users nh∆∞ng t·∫°o √°p l·ª±c l·ªõn! üî•
    """

    tasks = [OllamaUserBehavior]
    wait_time = between(0, 0.5)  # G·∫ßn nh∆∞ kh√¥ng wait
    weight = 1  # 5% users


# ============================================================================
# üé¨ Main Entry Point
# ============================================================================

if __name__ == "__main__":
    """
    Ch·∫°y local test ƒë·ªÉ verify tr∆∞·ªõc khi stress test l·ªõn!

    Usage:
        locust -f locustfile.py --host=http://localhost:8000
    """
    print(
        """
    üî• Ollama Load Test Ready!

    Commands:
        # Web UI mode (recommended):
        locust -f locustfile.py --host=http://localhost:8000

        # Headless mode (CI/CD):
        locust -f locustfile.py --host=http://localhost:8000 \\
               --users 100 --spawn-rate 10 --run-time 5m --headless

        # Distributed mode (multiple workers):
        locust -f locustfile.py --master --host=http://localhost:8000
        locust -f locustfile.py --worker --master-host=localhost

    Vibe: Testing nh∆∞ rockstar! üé∏
    """
    )
